{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from lightning.fabric.strategies import FSDPStrategy\n",
    "\n",
    "\n",
    "\n",
    "from generate.base import generate\n",
    "from lit_gpt import Tokenizer\n",
    "from lit_gpt.lora import GPT, Block, Config, merge_lora_weights\n",
    "from lit_gpt.utils import check_valid_checkpoint_dir, get_default_supported_precision, lazy_load, quantization\n",
    "from scripts.prepare_sql import generate_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 4\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "lora_query = True\n",
    "lora_key = True\n",
    "lora_value = True\n",
    "lora_projection = True\n",
    "lora_mlp = True\n",
    "lora_head = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_path: Path = Path(\"out/lora/sql_llama_cosine_scheduler/iter-025599-ckpt.pth\")\n",
    "checkpoint_dir: Path = Path(\"checkpoints/meta-llama/Llama-2-7b-chat-hf\")\n",
    "quantize: Optional[Literal[\"bnb.nf4\", \"bnb.nf4-dq\", \"bnb.fp4\", \"bnb.fp4-dq\", \"bnb.int8\", \"gptq.int4\"]] = \"bnb.nf4\"\n",
    "max_new_tokens: int = 100\n",
    "top_k: int = 200\n",
    "temperature: float = 0.8\n",
    "strategy: str = \"auto\"\n",
    "devices: int = 1\n",
    "precision: Optional[str] = 'bf16-true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A10G') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    }
   ],
   "source": [
    "precision = precision or get_default_supported_precision(training=False)\n",
    "\n",
    "if strategy == \"fsdp\":\n",
    "    strategy = FSDPStrategy(auto_wrap_policy={Block}, cpu_offload=False)\n",
    "fabric = L.Fabric(devices=devices, precision=precision, strategy=strategy)\n",
    "fabric.launch()\n",
    "\n",
    "check_valid_checkpoint_dir(checkpoint_dir)\n",
    "\n",
    "with open(checkpoint_dir / \"lit_config.json\") as fp:\n",
    "    config_params = dict(\n",
    "        r=lora_r,\n",
    "        alpha=lora_alpha,\n",
    "        dropout=lora_dropout,\n",
    "        to_query=lora_query,\n",
    "        to_key=lora_key,\n",
    "        to_value=lora_value,\n",
    "        to_projection=lora_projection,\n",
    "        to_mlp=lora_mlp,\n",
    "        to_head=lora_head,\n",
    "    )\n",
    "    config_params.update(**json.load(fp))\n",
    "    config = Config(**config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model 'checkpoints/meta-llama/Llama-2-7b-chat-hf/lit_model.pth' with {'org': 'meta-llama', 'name': 'Llama-2-7b-chat-hf', 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'intermediate_size': 11008, 'condense_ratio': 1, 'r': 4, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': True, 'to_value': True, 'to_projection': True, 'to_mlp': True, 'to_head': True}\n",
      "Time to instantiate model: 7.38 seconds.\n"
     ]
    }
   ],
   "source": [
    "if quantize is not None and devices > 1:\n",
    "    raise NotImplementedError\n",
    "if quantize == \"gptq.int4\":\n",
    "    model_file = \"lit_model_gptq.4bit.pth\"\n",
    "    if not (checkpoint_dir / model_file).is_file():\n",
    "        raise ValueError(\"Please run `python quantize/gptq.py` first\")\n",
    "else:\n",
    "    model_file = \"lit_model.pth\"\n",
    "checkpoint_path = checkpoint_dir / model_file\n",
    "\n",
    "fabric.print(f\"Loading model {str(checkpoint_path)!r} with {config.__dict__}\", file=sys.stderr)\n",
    "t0 = time.perf_counter()\n",
    "with fabric.init_module(empty_init=True), quantization(quantize):\n",
    "    model = GPT(config)\n",
    "fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time to load the model weights: 6.89 seconds.\n",
      "/opt/conda/envs/Parrot/lib/python3.10/site-packages/lightning/fabric/fabric.py:943: PossibleUserWarning: The model passed to `Fabric.setup()` has 66 parameters on different devices (for example 'transformer.wte.weight' on cuda:0 and 'lm_head.lora_A' on cpu). Since `move_to_device=True`, all parameters will be moved to the new device. If this is not desired, set `Fabric.setup(..., move_to_device=False)`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "with lazy_load(checkpoint_path) as checkpoint, lazy_load(lora_path) as lora_checkpoint:\n",
    "    checkpoint.update(lora_checkpoint.get(\"model\", lora_checkpoint))\n",
    "    model.load_state_dict(checkpoint, strict=quantize is None)\n",
    "fabric.print(f\"Time to load the model weights: {time.perf_counter() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "\n",
    "model.eval()\n",
    "merge_lora_weights(model)\n",
    "model = fabric.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are an expert SQL programmer and system administrator. Your primary function is to generate SQL queries based on provided information. Your tone is professional. Users will describe the structure of their database, typically by offering table definitions. An example of this is:\n",
      "CREATE TABLE A (b INTEGER).\n",
      "Should you not receive a specific definition, you are permitted to make educated assumptions about the database's structure.\n",
      "You can make any assumptions about the database's structure if some terminology is loose. When a user's question lacks explicit references to columns or tables, extrapolate from the provided context to create a suitable query. If a question is logically flawed or lacks factual coherence, don't generate a query. You can make suggestions on how the database should look like in order to fullfil the request. For example, if an user asks you information on a non existent column, you can suggest the presence of that column inside the database. If they agree with your hypotesis, generate the query.\n",
      "<</SYS>>\n",
      "\n",
      "The SQL database is composed by the following sql tables: CREATE TABLE food(name VARCHAR, calories INTEGER)\n",
      "What is the meaning of life. Use only Emojis. [/INST]SELECT name FROM food WHERE calories = (SELECT MAX(calories) FROM food) AS (name)\n"
     ]
    }
   ],
   "source": [
    "prompt: str = \"Select the \"\n",
    "input: str = \"CREATE TABLE food(name VARCHAR, calories INTEGER)\"\n",
    "tokenizer = Tokenizer(checkpoint_dir)\n",
    "sample = {\"instruction\": prompt, \"input\": input}\n",
    "prompt = generate_prompt(sample)\n",
    "# LLAMA encoding\n",
    "encoded = tokenizer.encode(prompt, device=model.device, bos=True)\n",
    "prompt_length = encoded.size(0)\n",
    "max_returned_tokens = prompt_length + max_new_tokens\n",
    "t0 = time.perf_counter()\n",
    "y = generate(\n",
    "    model,\n",
    "    encoded,\n",
    "    max_returned_tokens,\n",
    "    max_seq_length=max_returned_tokens,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    eos_id=tokenizer.eos_id,\n",
    ")\n",
    "t = time.perf_counter() - t0\n",
    "\n",
    "model.reset_cache()\n",
    "output = tokenizer.decode(y)\n",
    "fabric.print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringa = \"\\n[INST]  What is the meaning of life? You can only use Emojis.[/INST]\\n\"\n",
    "second_input = torch.concat((tokenizer.encode(output, eos=True, bos=True,device=model.device), tokenizer.encode(stringa, bos=True,device=model.device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are an expert SQL programmer and system administrator. Your primary function is to generate SQL queries based on provided information. Your tone is professional. Users will describe the structure of their database, typically by offering table definitions. An example of this is:\n",
      "CREATE TABLE A (b INTEGER).\n",
      "Should you not receive a specific definition, you are permitted to make educated assumptions about the database's structure.\n",
      "You can make any assumptions about the database's structure if some terminology is loose. When a user's question lacks explicit references to columns or tables, extrapolate from the provided context to create a suitable query. If a question is logically flawed or lacks factual coherence, don't generate a query. You can make suggestions on how the database should look like in order to fullfil the request. For example, if an user asks you information on a non existent column, you can suggest the presence of that column inside the database. If they agree with your hypotesis, generate the query.\n",
      "<</SYS>>\n",
      "\n",
      "The SQL database is composed by the following sql tables: CREATE TABLE food(name VARCHAR, calories INTEGER)\n",
      "What is the meaning of life. Use only Emojis. [/INST]SELECT name FROM food WHERE calories = (SELECT MAX(calories) FROM food) AS (name) \n",
      "[INST]  What is the meaning of life? You can only use Emojis.[/INST]\n",
      "üçîüê∂üèºüèôÔ∏èüíë\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "y = generate(\n",
    "    model,\n",
    "    second_input,\n",
    "    max_returned_tokens,\n",
    "    max_seq_length=max_returned_tokens,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    eos_id=tokenizer.eos_id,\n",
    ")\n",
    "t = time.perf_counter() - t0\n",
    "\n",
    "model.reset_cache()\n",
    "output = tokenizer.decode(y)\n",
    "fabric.print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
